{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling with the new Hermes\n",
    "\n",
    "Copied and ajdusted from https://github.com/abacaj/openhermes-function-calling/tree/main\n",
    "Model: teknium/OpenHermes-2.5-Mistral-7B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "535201b2-95b7-4a5c-aa1b-5f95f9e695ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0050830841064453125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7e33a11a9940debc870faa45abadba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16,device_map=\"auto\")\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4059915c-ce53-4178-9707-a930428254ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "fn_call_syntax = \"\"\"{\"name\": \"function_name\", \"arguments\": {\"arg_1\": \"value_1\", \"arg_2\": value_2, ...}}\"\"\"\n",
    "\n",
    "def generate_functions_prompt(query, functions=[]):\n",
    "    func_string = \"\\n\\n\".join([json.dumps(fn) for fn in functions])\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "You are a helpful assistant with access to the following functions:\n",
    "\n",
    "{func_string}\n",
    "\n",
    "To use these functions respond with:\n",
    "<functioncall> {fn_call_syntax} </functioncall>\n",
    "\n",
    "Edge cases you must handle:\n",
    "- If there are no functions that match the user request, you will respond politely that you cannot help.<|im_end|>\n",
    "<|im_start|>user\n",
    "{query}<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "    return  prompt\n",
    "\n",
    "def generate(prompt):\n",
    "    return pipe(generate_functions_prompt(prompt),max_new_tokens=512,do_sample=False,return_full_text=False, pad_token_id=pipe.tokenizer.eos_token_id)[0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant with access to the following functions:\n",
      "\n",
      "{\"name\": \"call_uber\", \"description\": \"Find suitable ride for customers given the location, type of ride, and the amount of time the customer is willing to wait as parameters\", \"parameters\": [{\"name\": \"loc\", \"type\": \"string\", \"description\": \"location of the starting place of the uber ride\"}, {\"name\": \"type\", \"type\": \"string\", \"enum\": [\"plus\", \"comfort\", \"black\"], \"description\": \"types of uber ride user is ordering\"}, {\"name\": \"time\", \"type\": \"number\", \"description\": \"the amount of time in minutes the customer is willing to wait\"}]}\n",
      "\n",
      "{\"name\": \"get_current_weather\", \"description\": \"Gets the current weather for a given location\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the users location.\"}}, \"required\": [\"location\", \"format\"]}}\n",
      "\n",
      "To use these functions respond with:\n",
      "<functioncall> {\"name\": \"function_name\", \"arguments\": {\"arg_1\": \"value_1\", \"arg_2\": value_2, ...}} </functioncall>\n",
      "\n",
      "Edge cases you must handle:\n",
      "- If there are no functions that match the user request, you will respond politely that you cannot help.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the weather in San Francisco?<|im_end|>\n",
      "<|im_start|>assistant\n"
     ]
    }
   ],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"call_uber\",\n",
    "        \"description\": \"Find suitable ride for customers given the location, type of ride, and the amount of time the customer is willing to wait as parameters\",\n",
    "        \"parameters\":  [\n",
    "            {\"name\": \"loc\", \"type\": \"string\", \"description\": \"location of the starting place of the uber ride\"},\n",
    "            {\"name\":\"type\", \"type\": \"string\", \"enum\": [\"plus\", \"comfort\", \"black\"], \"description\": \"types of uber ride user is ordering\"},\n",
    "            {\"name\": \"time\", \"type\": \"number\", \"description\": \"the amount of time in minutes the customer is willing to wait\"}\n",
    "            ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Gets the current weather for a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"format\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"location\", \"format\"],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "print(generate_functions_prompt(\"What is the weather in San Francisco?\",functions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<functioncall> {\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"New York, NY\", \"format\": \"celsius\"}} </functioncall>\n",
      "\n",
      "<functioncall> {\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"New York\", \"format\": \"celsius\"}} </functioncall>\n",
      "\n",
      "I'm sorry, I don't have the ability to provide weather information at the moment. Please try again later.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the weather in New York?\"\n",
    "# get_current_weather last function in list\n",
    "prompt = generate_functions_prompt(query,functions=functions)\n",
    "print(generate(prompt))\n",
    "# get_current_weather first function in list\n",
    "prompt = generate_functions_prompt(query,functions=functions[::-1])\n",
    "print(generate(prompt))\n",
    "# Only get_current_weather in list\n",
    "prompt = generate_functions_prompt(query,functions=[functions[::-1][0]])\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<functioncall> {\"name\": \"call_uber\", \"arguments\": {\"loc\": \"Berkeley, 94704\", \"type\": \"plus\", \"time\": 10}} </functioncall>\n",
      "\n",
      "<functioncall> {\"name\": \"call_uber\", \"arguments\": {\"loc\": \"Berkeley, CA 94704\", \"type\": \"plus\", \"time\": 10}} </functioncall>\n",
      "\n",
      "<functioncall> {\"name\": \"call_uber\", \"arguments\": {\"loc\": \"Berkeley, 94704\", \"type\": \"plus\", \"time\": 10}} </functioncall>\n"
     ]
    }
   ],
   "source": [
    "query = \"Call me an plus Uber in Berkeley at zipcode 94704 in 10 minutes\"\n",
    "# call_uber first function in list\n",
    "prompt = generate_functions_prompt(query,functions=functions)\n",
    "print(generate(prompt))\n",
    "# call_uber last function in list\n",
    "prompt = generate_functions_prompt(query,functions=functions[::-1])\n",
    "print(generate(prompt))\n",
    "# Only call_uber in list\n",
    "prompt = generate_functions_prompt(query,functions=[functions[0]])\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
